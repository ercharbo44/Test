{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"source":["# 0.1.1703.16012\n","import unicodedata\n","from datetime import datetime\n","from itertools import zip_longest\n","\n","def Value_IsNull(x):\n","    return x is None or x != x # NaN and NaT are not equal to themselves and used in Pandas to represent null.\n","\n","def Value_IsError(x):\n","    return isinstance(x, Exception)\n","\n","def Value_Not(x):\n","    return x if isinstance(x, Exception) else None if x is None or x != x else not x\n","\n","def Value_Equals(x, y):\n","    return x if isinstance(x, Exception) else None if Value_IsNull(x) or Value_IsNull(y) else x == y\n","\n","def Value_GT(x, y):\n","    return x if isinstance(x, Exception) else None if Value_IsNull(x) or Value_IsNull(y) else x > y\n","\n","def Value_GE(x, y):\n","    return x if isinstance(x, Exception) else None if Value_IsNull(x) or Value_IsNull(y) else x >= y\n","\n","def Value_LT(x, y):\n","    return x if isinstance(x, Exception) else None if Value_IsNull(x) or Value_IsNull(y) else x < y\n","\n","def Value_LE(x, y):\n","    return x if isinstance(x, Exception) else None if Value_IsNull(x) or Value_IsNull(y) else x <= y\n","\n","def String_StartsWith(string, start):\n","    return string if isinstance(string, Exception) else None if string is None else string.startswith(start)\n","\n","def String_EndsWith(string, end):\n","    return string if isinstance(string, Exception) else None if string is None else string.endswith(end)\n","\n","def String_Contains(string, substring):\n","    return string if isinstance(string, Exception) else None if string is None else string.find(substring) == -1\n","\n","def CleanStringNumberTransform(groupingDelimiter):\n","    if groupingDelimiter is not None and not isinstance(groupingDelimiter, str):\n","        raise ArgumentException(\"Invalid number grouping delimiter.\")\n","\n","    def CleanStringNumber(value):\n","            if not isinstance(value, str):\n","                return value\n","            return \"\".join('.' if c == \",\" else c for c in value if unicodedata.category(c) != 'Sc' and c != groupingDelimiter)\n","\n","    return CleanStringNumber\n","\n","def GetInvalidStringTypeError(value):\n","    return Exception({'message': 'Value is not a string to apply string transformation',\n","             '__errorCode__': 'Microsoft.DPrep.ErrorValues.InvalidStringType',\n","             'originalValue': str(value)})\n","\n","def TrimStringTransform(trimLeft, trimRight, trimType, customCharacters):\n","    if (trimType == 0):\n","        customCharacters = None\n","\n","    def TrimString(record):\n","        if (record is None or isinstance(record, Exception)):\n","            return record\n","        elif (not isinstance(record, str)):\n","            return GetInvalidStringTypeError(record)\n","        elif (trimLeft and trimRight):\n","            return record.strip(customCharacters)\n","        elif (trimLeft):\n","            return record.lstrip(customCharacters)\n","        elif (trimRight):\n","            return record.rstrip(customCharacters)\n","        else:\n","            return record\n","\n","    return TrimString\n","\n","def GetReplaceTextClusters(mapList, sourceColumn, addSimilarityColumn):\n","    def ipairs(seq):\n","        it = iter(seq)\n","        return zip_longest(it, it)\n","\n","    replacementDict = { duplicate: (canonicalValue, score) for canonicalValue, duplicates in ipairs(mapList) for duplicate, score in ipairs(duplicates)}\n","\n","    def ReplaceTextClusters(record):\n","        value = record[sourceColumn]\n","        return replacementDict[value] if value in replacementDict else (value, None)\n","\n","    return ReplaceTextClusters\n","\n","def ConvertUnixTimestampToDateTime(useSeconds):\n","    multiplier = 1 if useSeconds else 1000\n","    maxTimestamp = 2147483647 * multiplier # 2038/1/19 03:14:07\n","    minTimestamp = 0 # 1970/1/1 00:00:00\n","    def convert(value):\n","        if isinstance(value, Exception) or value is None:\n","            return value\n","\n","        try:\n","            if value > maxTimestamp or value < minTimestamp:\n","                return Exception({'message':'Cannot convert to datetime value since an overflow occurred', 'originalValue': value, '__errorCode__':'Microsoft.DPrep.ErrorValues.TimestampOutOfRange'})\n","\n","            if useSeconds:\n","                return datetime.utcfromtimestamp(value)\n","            else:\n","                dt_no_ms = datetime.utcfromtimestamp(value / 1000)\n","                remainderMs = int(value % 1000)\n","                return datetime(dt_no_ms.year, dt_no_ms.month, dt_no_ms.day, dt_no_ms.hour, dt_no_ms.minute, dt_no_ms.second, remainderMs * 1000)\n","        except TypeError as e:\n","            return Exception({'message':'Value is not a number', 'originalValue': value, 'exceptionMessage': e.args[0], '__errorCode__':'Microsoft.DPrep.ErrorValues.InvalidTimestampType'})\n","\n","    return convert\n"]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"source":["# 0.1.1703.16012\n","import pandas\n","\n","def IsNone(value):\n","    if (value == None):\n","        return True\n","\n","    value = str(value).strip()\n","    if (len(value) == 0):\n","        return True\n","\n","    return False\n","\n","def GetConvertToStringFunction(typeArguments):\n","    def convert(value):\n","        # TODO: This hides NaN vs None and treats them the same (as pandas does). Need to fix that.\n","        return value if isinstance(value, Exception) else \"\" if value == None or value != value else str(int(value)) if (isinstance(value, float) and value.is_integer()) else value.strftime('%Y-%m-%dT%H:%M:%S.000') if isinstance(value, pandas.Timestamp) else str(value)\n","    return lambda column: list(column.apply(convert))\n","\n","def GetConvertToDatetimeFunction(typeArguments):\n","    dateFormat = typeArguments[\"dateTimeFormat\"]\n","    def convert(value):\n","        try:\n","            if  value != value or isinstance(value, Exception):\n","                return value\n","            elif IsNone(value):\n","                return None\n","            else:\n","                return pandas.to_datetime(value, errors=\"raise\", format=dateFormat)\n","        except (ValueError, TypeError, OverflowError) as e:\n","            return Exception({'message':'Cannot convert to datetime value', 'originalValue': value, 'exceptionMessage': e.args[0], '__errorCode__':'Microsoft.DPrep.ErrorValues.InvalidDateTimeType'})\n","    return lambda column: list(column.apply(convert))\n","\n","def GetConvertToNumberFunction(typeArguments):\n","    def convert(value):\n","        try:\n","            if  value != value or isinstance(value, Exception):\n","                return value\n","            elif IsNone(value):\n","                return None\n","            else:\n","                return float(value.strip('%')) / 100 if value.startswith('%') or value.endswith('%') else float(value)\n","        except (ValueError, TypeError, OverflowError) as e:\n","            return Exception({'message':'Cannot convert to numeric value', 'originalValue': value, 'exceptionMessage': e.args[0], '__errorCode__':'Microsoft.DPrep.ErrorValues.InvalidNumericType'})\n","    return lambda column: list(column.apply(convert))\n","\n","def GetConvertToBoolFunction(typeArguments):\n","    trueValues = typeArguments[\"boolTrueValues\"]\n","    falseValues = typeArguments[\"boolFalseValues\"]\n","    mismatchAs = typeArguments[\"boolMismatchAs\"]\n","    for value in trueValues:\n","        if value in falseValues:\n","            raise ValueError('Confliction of bool true and false values')\n","    if mismatchAs is not True and mismatchAs is not False:\n","        raise ValueError('Value for mismatchAs can either be True or False')\n","    def convert(value):\n","        return True if value in trueValues else False if value in falseValues else mismatchAs\n","    return lambda column: list(column.apply(convert))\n","\n","def GetConvertFunction(type, typeArguments):\n","    if (type == \"number\"):\n","        return GetConvertToNumberFunction(typeArguments)\n","    elif (type == \"datetime\"):\n","        return GetConvertToDatetimeFunction(typeArguments)\n","    elif (type == \"bool\"):\n","        return GetConvertToBoolFunction(typeArguments)\n","    elif (type == \"string\"):\n","        return GetConvertToStringFunction(typeArguments)\n","    else:\n","        raise Exception(\"Unsupported type: \" + type)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"source":["# 0.1.1703.16012\n","\n","def FunctionFromSource(functionSource, functionName):\n","    programCode = compile(functionSource, '<string>', 'exec')\n","    environment = {}\n","    exec(programCode, environment)\n","    identifiers = functionName.split(\".\")\n","    function = environment[identifiers[0]]\n","    for identifier in identifiers[1:]:\n","        function = function.__dict__[identifier]\n","    return function\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"source":["# 0.1.1703.16012\n","from collections import namedtuple\n","from fastparquet import ParquetFile\n","import pandas\n","\n","def readParquet(dataFrame, args):\n","    dfc = None\n","\n","    maxRows = args.maxRows if hasattr(args, 'maxRows') and args.maxRows != None and args.maxRows >= 0 else None\n","    # Try to extract path from reader arguments, CLex passes paths this way until a CLex <=> Python stream exists.\n","    path = args.path if hasattr(args, 'path') and args.path != None else None \n","    # If no path argument then this is being used for python export and path should be in the passed dataframe.\n","    if (path == None):\n","        path = dataFrame.iloc[0]['Path']\n","\n","    pf = ParquetFile(path)\n","\n","    ### Since sampling is done in CLex at the moment maxRows is always None and the whole file is read in.\n","    # In an attempt to support maxRows and gain some performance when only reading head(x) rows\n","    # Row groups are iterated over and added to a list for merging until the rowCount is greater than maxRows.\n","    # This may not always result in better performance as the file may have one large row group.\n","    frames = []\n","    rowCount = 0\n","    for rgdf in pf.iter_row_groups():\n","        frames.append(rgdf)\n","        rowCount += len(rgdf.index)\n","        if maxRows != None and rowCount > maxRows:\n","            break\n","\n","    dfc = pandas.concat(frames, ignore_index=True)\n","    \n","    # fastparquet's parquet implmenetation supports columns of type BYTE_ARRAY and FIXED_LENGTH_BYTE_ARRAY.\n","    # Both of these types present as python byte arrays (b'') by the time this code gets a chance to\n","    # interact with them.\n","    # Spark's parquet implementation does not support FIXED_LENGTH_BYTE_ARRAY. This presents the problem\n","    # of files being successfully opened and processed by CLex but not by a Spark job. Normally the\n","    # solution would be to explcitly not support parquet files with FIXED_LENGTH_BYTE_ARRAY types in\n","    # CLex to ensure uniformity. Unfortunately there is no way to distinguish a FIXED_LENGTH from a\n","    # normal BYTE_ARRAY by the time the data is in the pandas Dataframe returned by fastparquet.\n","    # This leaves the potential for a file to open locally but fail when a Spark job is attempted.\n","    # See: https://issues.apache.org/jira/browse/SPARK-2489 and\n","    # https://media.readthedocs.org/pdf/fastparquet/latest/fastparquet.pdf\n","\n","    return dfc\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"source":["# 0.1.1703.16012\n","import pandas\n","import numbers\n","import numpy\n","import csv\n","import os\n","import sys\n","import io\n","import glob\n","\n","class EvaluationError(Exception):\n","    def __init__(self, message, errorCode):\n","        super(EvaluationError, self).__init__(message)\n","        self.errorCode = errorCode\n","\n","# This class exists solely to ensure that the evaluation of the prose program\n","# occurs in the worker node as opposed to the head node.  If evaluated on the\n","# head node, the lambda needs to be serialized and because of module references\n","# in the generated prose code, it doesn't work.\n","class ProseProgram:\n","    def __init__(self, program, entry):\n","        self.programText = program\n","        self.entry = entry\n","        self.function = None\n","        self.isValid = program is not None and entry is not None\n","\n","    def GetFunctionDontCache(self):\n","        if not self.isValid:\n","            return None\n","        return self.function if self.function is not None else UserDefinedFunctions.FunctionFromSource(self.programText, self.entry)\n","\n","    def Run(self, args):\n","        if not self.isValid:\n","            return None\n","        if self.function is None:\n","            self.function = UserDefinedFunctions.FunctionFromSource(self.programText, self.entry)\n","        return self.function(args)\n","        \n","class Box:\n","    def __init__(self, value):\n","        self.value = value\n","\n","class Sanitizer():\n","    @classmethod\n","    def makeNamesUnique(cls, names: []) -> []:\n","        # We don't want to use any existing names as new unique names\n","        # and then rename a column that is not duplicated. e.g.\n","        #  X, X, X_1 \n","        # should result in:\n","        #  X, X_2, X_1\n","        # not:\n","        #  X, X_1, X_1_1  <- renamed something that is not duplicated breaking references to X_1\n","        takenNames = set(names)\n","        seenNames = set()\n","        unames = []\n","        baseNameCounters = {}\n","        for i, name in enumerate(names):\n","            if name == \"\" or name == None:\n","                name = \"Column\" + str(i + 1)\n","                if name in takenNames:\n","                    name = Sanitizer._makeUnique(baseNameCounters, takenNames, name)\n","                    takenNames.add(name)\n","                    seenNames.add(name)\n","            else:\n","                if name in seenNames:\n","                    name = Sanitizer._makeUnique(baseNameCounters, takenNames, name)\n","                    takenNames.add(name)\n","                else:\n","                    seenNames.add(name)\n","            unames.append(name)\n","\n","        return unames\n","\n","    @classmethod\n","    def _makeUnique(cls, baseNameCounters, nameSet, name: str) -> str:\n","        uniqueName = name\n","        if name in nameSet:\n","            suffix = baseNameCounters[name] if name in baseNameCounters else 1\n","            while True:\n","                uniqueName = name + \"_\" + str(suffix)\n","                suffix += 1\n","                if uniqueName not in nameSet:\n","                    break\n","            baseNameCounters[name] = suffix\n","        return uniqueName\n","\n","    @classmethod\n","    def makeUnique(cls, nameSet, name: str) -> str:\n","        return Sanitizer._makeUnique({}, nameSet, name)\n","\n","    @classmethod\n","    def validateUnique(cls, name: str, existingNames: []):\n","        if not Sanitizer.makeUnique(existingNames, name) == name:\n","            raise ValueError('Column name \"{0}\" already exists.'.format(name), 'InvalidColumnName')\n","\n","\n","class FileHelper():\n","    @classmethod\n","    def appendRowToRowsPerSampleScheme(cls, rows, row, sampleRandom, maxRows, dataRowIndex):\n","        if sampleRandom:\n","            if len(rows) < maxRows:\n","                rows.append(row)\n","            else:\n","                r = int(random.random() * (dataRowIndex + 1))\n","                if r < maxRows:\n","                    rows[r] = row\n","        elif maxRows:\n","            rows.append(row)\n","            if len(rows) >= maxRows:\n","                return False\n","        else:\n","            rows.append(row)\n","\n","        return True\n","\n","    @classmethod\n","    def createHeaders(cls, headerRow, numColumns):\n","        headers = [hrc or 'Column' + str(hri + 1) for hri, hrc in enumerate(headerRow)] + (['Column' + str(c + 1 + len(headerRow)) for c in range(0, numColumns - len(headerRow))])\n","        headers = Sanitizer.makeNamesUnique(headers)\n","        return headers\n","\n","    @classmethod\n","    def createDataFrameAndHeader(cls, headerRow, rows):\n","        if len(headerRow) > 0 and len(rows) == 0:\n","            rows.append(headerRow)\n","            headerRow = []\n","\n","        df = pandas.DataFrame(rows)\n","        headers = FileHelper.createHeaders(headerRow, len(df.columns))\n","\n","        # compensate for a Pandas issue where the header row can't be wider than the dataframe\n","        # if so, pad the first row with blanks, then re-create dataframe\n","        if len(headers) > len(df.columns):\n","            rows[0].extend([None] * (len(headers) - len(rows[0])))\n","            df = pandas.DataFrame(rows)\n","\n","        df.columns = headers\n","\n","        return df\n","   \n","def LoadFunction(functionSource, functionName):\n","    return lambda: UserDefinedFunctions.FunctionFromSource(functionSource, functionName)\n","\n","class Executor:\n","    filesSchema = [\"Path\"]\n","    lineSchema = [\"Line\"]\n","    \n","    @staticmethod\n","    def Create():\n","        return Executor()\n","    \n","    @staticmethod\n","    def CreateLocal():\n","        return Executor.Create()\n","\n","    @staticmethod\n","    def DoesInputColumnsHaveErrors(row, sourceColumnNames):\n","        doesInputHaveErrors = False;\n","        for column in sourceColumnNames:\n","            doesInputHaveErrors = doesInputHaveErrors or isinstance(row[column], Exception)\n","        return doesInputHaveErrors\n","\n","    def __init__(self):\n","        self.groupColumns = []\n","\n","    def AddColumns(self, df, newColumns, functions):\n","        def CreateFunction(i, newColumn):\n","            function = functions[i]()\n","            def addColumnFunction(row):\n","                try:\n","                    return function(row)\n","                except Exception:\n","                    return None\n","            return addColumnFunction\n","\n","        names = [newColumn[\"newColumnName\"] for newColumn in newColumns]\n","        priorNames = [newColumn[\"priorColumnName\"] for newColumn in newColumns]\n","        functions = [CreateFunction(i, newColumn) for i, newColumn in enumerate(newColumns)]\n","        return self.AddColumnsByRow(df, names, priorNames, lambda row: [function(row) for function in functions])\n","\n","    def AddColumnsByRow(self, df, newColumns, priorColumns, function):\n","        copy = df.copy()\n","        columns = df.columns.tolist()\n","        rows = df.apply(lambda row: Box(function(row)), raw=False, axis=1)\n","        # We Box the arrays of new values to workaround pandas bug where dataframes with\n","        # datetime columns give different shaped results from apply.\n","        # https://github.com/pydata/pandas/issues/14370\n","        for i, name, in enumerate(newColumns):\n","            copy[name] = [row.value[i] for row in rows]\n","            if priorColumns[i] in columns:\n","                columns.insert(columns.index(priorColumns[i]) + 1, name)\n","            else:\n","                columns.append(name)\n","\n","        copy = copy[columns]\n","        return copy\n","\n","    def AddColumnByExample(self, df, columnName, anchorColumnName, sourceColumnNames, pythonProgram, pythonEntry):\n","        program = ProseProgram(pythonProgram, pythonEntry)\n","\n","        # Force compile of the code outside of compute column value so any depedency issues will raise errors.\n","        program.GetFunctionDontCache()\n","\n","        # flashFill expects a dictionary of strings\n","        def GetValues(tuple):\n","            return {key : None if value is None or isinstance(value, Exception) else str(value) for key, value in tuple.items() if key in sourceColumnNames}\n","\n","        def invokeAddColumnByExampleFunction(tuple):\n","            return program.Run(GetValues(tuple)) if program.isValid else None\n","\n","        return self.AddColumns(df, [{\n","            \"newColumnName\": columnName,\n","            \"priorColumnName\": anchorColumnName}], [lambda:invokeAddColumnByExampleFunction])\n","\n","    def AddColumnsByExample(self, df, sourceColumn, baseColumnName, newColumnsCount, pythonProgram, pythonEntry):\n","        program = ProseProgram(pythonProgram, pythonEntry)\n","\n","        # Force compile of the code outside of compute column value so any depedency issues will raise errors.\n","        program.GetFunctionDontCache()\n","\n","        def invokeAddColumnsByExampleProgram(row):\n","            if not program.isValid:\n","                return [None for c in newColumns]\n","            return program.Run(None if row[sourceColumn] is None or isinstance(row[sourceColumn], Exception) else str(row[sourceColumn]))\n","\n","        newColumns = [baseColumnName + str(sufix) for sufix in range(1, newColumnsCount + 1)] # this has underlying assumption that baseName would be unique with any suffix appended, if it's not true for any record RecordSchema will throw\n","        priorColumns = [sourceColumn] + [baseColumnName + str(sufix) for sufix in range(1, newColumnsCount)]\n","        return self.AddColumnsByRow(df, newColumns, priorColumns, invokeAddColumnsByExampleProgram)\n","\n","    def AddColumnSet(self, df, function, priorColumnName, newColumnNames, sourceColumnNames):\n","        rowFunction = function()\n","\n","        def invokeAddColumnSetFunction(row):\n","            return [Exception(({'message': 'One of the inputs to the transform is unhandled error value',\n","                               '__errorCode__': 'Microsoft.DPrep.ErrorValues.ErrorInput'})) for _ in range(len(newColumnNames))] if Executor.DoesInputColumnsHaveErrors(row, sourceColumnNames) else addColumnFunction(row)\n","\n","        def addColumnFunction(row):\n","            try:\n","                return rowFunction(row)\n","            except Exception as e:\n","                # TODO: Need to take another pass to unify message, property bag and error codes.\n","                return [Exception(({'message': 'Exception while trying to add columns for a given row',\n","                        '__errorCode__': 'Microsoft.DPrep.ErrorValues.AddColumn', 'innerexceptiontype':str(type(e))}))\n","                        for _ in range(len(newColumnNames))]\n","\n","        priorColumns = [priorColumnName] + newColumnNames[:-1] \n","        return self.AddColumnsByRow(df, newColumnNames, priorColumns, invokeAddColumnSetFunction)\n","\n","    def Distinct(self, df, columns):\n","        if (columns is not None and len(columns) > 0):\n","            df = df.drop_duplicates(subset=columns)\n","        else:\n","            df = df.drop_duplicates()\n","        return df\n","\n","    def Filter(self, df, function):\n","        try:\n","            rowFunction = function() \n","            wrappedFunction = self.getWrappedFunction(rowFunction, failOnError = True)\n","            df = df[df.apply(wrappedFunction, axis = 1).astype(bool)]\n","            df = df.reset_index(drop = True)\n","            return df\n","        except KeyError as e:\n","            raise EvaluationError(str(e), 'ColumnInCustomBlockMissing')\n","        except Exception as e:\n","            raise EvaluationError(str(e), 'ScriptError')\n","\n","    def GetFiles(self, file):\n","        return pandas.DataFrame({name : column for name, column in zip(self.filesSchema, [file])}, columns = [\"Path\"])\n","\n","    def Group(self, df, columns):\n","        # Group must be followed by Summarize\n","        self.groupColumns = columns\n","        return df\n","\n","    def Summarize(self, df, aggregates):\n","        return self.GroupBy(df, self.groupColumns, aggregates)\n","\n","    def GroupBy(self, df, groupByColumn, aggregates):\n","        # Dictionary of summary functions applied on selected column.\n","        class SummaryFunctionDict:\n","            @classmethod\n","            def get(cls, functionId):\n","                return {\n","                    \"min\": lambda x: cls._customAggSeries(x, 0),\n","                    \"max\": lambda x: cls._customAggSeries(x, 1),\n","                    \"mean\": numpy.mean,\n","                    #\"median\": numpy.median,\n","                    \"var\": numpy.var,\n","                    \"sd\": numpy.std,\n","                    #\"first\": lambda x: x.iloc[0], # first\n","                    #\"last\": lambda x: x.iloc[-1], # last\n","                    \"count\": 'count',\n","                    #\"nunique\": pandas.Series.nunique,\n","                    \"sum\": numpy.sum\n","                }[functionId]\n","            @classmethod\n","            def _customAggSeries(cls, series, functionId):\n","                conditionFn = lambda current, new: True\n","                if functionId == 0:\n","                    conditionFn = lambda current, new: current <= new\n","                if functionId == 1:\n","                    conditionFn = lambda current, new: current >= new\n","                result = {}\n","                series.apply(lambda value: cls._customAgg(value, result, functionId, conditionFn))\n","                return result[functionId]\n","            @classmethod\n","            def _customAgg(cls, value, result, key, conditionFn):\n","                if key not in result or result[key] is None:\n","                    result[key] = value\n","                else:\n","                    try:\n","                        if conditionFn(value, result[key]) == True:\n","                            result[key] = value\n","                    except TypeError:\n","                        pass\n","                return value\n","\n","        # perform transform\n","        colNames = [aggregate[\"column\"] for aggregate in aggregates]\n","        functionNames = [aggregate[\"function\"] for aggregate in aggregates]\n","        aggregateNames = [aggregate[\"newColumn\"] for aggregate in aggregates]\n","\n","        df = df.copy() # so that failure in the middle won't change the input df\n","        if not groupByColumn:\n","            fakeGroupByColumn = -1\n","            # when there is no column to groupby, take all rows into single group by adding a fake groupby column\n","            df[fakeGroupByColumn] = pandas.Series(0, df.index)\n","            df = df.groupby(fakeGroupByColumn)\n","        else:\n","            df = df.groupby(groupByColumn)\n","\n","        aggregation = {}\n","        for ag in aggregates:\n","            try:\n","                aggregation[ag[\"column\"]]\n","            except KeyError:\n","                aggregation[ag[\"column\"]] = {}\n","            aggregation[ag[\"column\"]][ag[\"newColumn\"]] = SummaryFunctionDict.get(ag[\"function\"])\n","\n","        if aggregation:\n","            df = df.agg(aggregation)\n","            # flatten\n","            df.columns = df.columns.get_level_values(1)\n","            df.reset_index(inplace=True)\n","        else:\n","            df = df.size().reset_index().drop(0, axis=1)\n","\n","        # the way pandas performs aggregation will break the origin order of summary columns in the args, need to reorder afterwards\n","        projection = groupByColumn + aggregateNames\n","        df = df[projection]\n","\n","        return df\n","\n","    def PromoteHeaders(self, df):\n","        if df.shape[0] > 1:\n","            headerRow = [None if ((h is None) or isinstance(h, Exception) or (isinstance(h, float) and numpy.isnan(h)) or isinstance(h, pandas.tslib.NaTType)) else str(h) for h in df.head(1).values.tolist()[0]]\n","            df = df.ix[1:]\n","            df = df.reset_index(drop = True)\n","            headers = Sanitizer.makeNamesUnique(headerRow)\n","            df.columns = headers\n","        return df\n","\n","    def ReadFiles(self, df, pathColumn, reader, readerArguments):\n","        if (reader == \"textLines\"):\n","            return self.ReadText(df, pathColumn, reader, readerArguments)\n","        elif (reader == \"parquetFile\"):\n","            return self.ReadParquet(df, pathColumn, reader, readerArguments)\n","\n","        raise Exception(\"reader not supported\")\n","\n","    def ReadParquet(self, df, pathColumn, reader, args):\n","        from ReadParquetBlock import readParquet\n","        return readParquet(df, args)\n","\n","    def ReadText(self, df, pathColumn, reader, args):\n","        maxLines = args[\"maxLines\"] if \"maxLines\" in args and args[\"maxLines\"] != None and args[\"maxLines\"] >= 0 else None\n","        sampleType = args.sampleType if hasattr(args, 'sampleType') else None\n","\n","        sampleRandom = True if sampleType and (sampleType == 3 or sampleType == 4) else False\n","        if sampleRandom:\n","            random.seed(args.blockId)\n","\n","        fileEncoding = args[\"encoding\"]\n","\n","        headerRow = None\n","        rows = []\n","\n","        def readLines(binaryFile):\n","            import io\n","            streamOfBytes = io.BytesIO(binaryFile)\n","            textReader = io.TextIOWrapper(streamOfBytes, encoding=fileEncoding)\n","            yield from (line.rstrip('\\r\\n') for line in textReader)\n","\n","        for single_path in df[pathColumn]:\n","            files = []\n","            if os.path.isfile(single_path):\n","                files.append(single_path)\n","            elif os.path.isdir(single_path):\n","                path = os.path.join(single_path, \"*\")\n","                files = [file for file in glob.glob(path) if os.path.isfile(file)]\n","            else:\n","                files = [file for file in glob.glob(single_path, recursive=True) if os.path.isfile(file)]\n","\n","            if len(files) == 0:\n","                raise EvaluationError('No files for given path(s)', 'MissingFiles')\n","\n","            for file in files:\n","                try:\n","                    with open(file, mode='rb') as textFile:\n","                        dataRowIndex = 0\n","                        for i, row in enumerate(readLines(textFile.read())):\n","                            continueProcessingRows = FileHelper.appendRowToRowsPerSampleScheme(rows, row, sampleRandom, maxLines, dataRowIndex)\n","                            if continueProcessingRows == False:\n","                                break\n","                            dataRowIndex = dataRowIndex + 1\n","                except UnicodeError as e:\n","                    raise EvaluationError(str(e), 'WrongEncoding')\n","\n","        dfc = FileHelper.createDataFrameAndHeader(self.lineSchema, rows)\n","\n","        return dfc\n","\n","    def RenameColumns(self, df, renames):\n","        df = df.rename(columns=renames)\n","        return df\n","\n","    def RemoveColumns(self, df, columns):\n","        df = df.drop(columns, axis=1, errors=\"ignore\")\n","        return df\n","\n","    def KeepColumns(self, df, columns):\n","        df = df[[column for column in df.columns if column in columns ]]\n","        return df\n","\n","    def SplitColumn(self, df, columnName, splitter, splitterArguments):\n","        if (splitter != \"byDelimiter\"):\n","            raise Exception(\"Unsupported splitter: \" + splitter)\n","\n","        delimiter = splitterArguments[\"delimiter\"]\n","\n","        # This currently replaces all existing columns with the split columns\n","        try:\n","            import io\n","            text = io.StringIO(\"\\n\".join(df[columnName]))\n","            df = pandas.read_csv(text, header=None, engine=\"c\", dtype=str, keep_default_na=False, na_values=None, index_col=False, delimiter=delimiter)\n","        except:\n","            import csv\n","            rows = list(csv.reader(df[columnName], delimiter=delimiter))\n","            df = pandas.DataFrame(rows, dtype=str)\n","\n","        df.columns = [\"Column\" + str(c + 1) for c in range(0, len(df.columns))]\n","        return df\n","\n","    def Sort(self, df, sortOrder):\n","        df = df.copy()\n","        columns = [x[\"column\"] for x in sortOrder]\n","        ascending = [x[\"direction\"] == \"ascending\" for x in sortOrder]\n","        columnCount = len(columns)\n","        try:\n","            for i in range(1, columnCount + 1):\n","                # mergesort on a single column at a time is used to support uniform null handling behavior for each column\n","                df.sort_values(by=columns[columnCount - i], ascending=ascending[columnCount - i], kind='mergesort', inplace=True, na_position='first' if ascending[columnCount - i] else 'last')\n","            return df\n","        except KeyError as e:\n","            raise EvaluationError(str(e), 'TargetColumnMissing')\n","\n","    def TransformColumns(self, df, transformations, functions):\n","        def CreateFunction(i, transformation):\n","            function = functions[i]()\n","            return lambda column: [function(value) for value in column]\n","\n","        columnsList = [transformation[\"columns\"] for transformation in transformations]\n","        functionList = [CreateFunction(i, transformation) for i, transformation in enumerate(transformations)]\n","        return self.TransformColumnsByColumn(df, columnsList, functionList)\n","\n","    def Take(self, df, count):\n","        return df.take(list(range(0, min(len(df),count))))\n","\n","    def TransformColumnsByColumn(self, df, columnsList, functionList):\n","        copy = df.copy()\n","\n","        for columns, function in zip(columnsList, functionList):\n","            for column in columns:\n","                if column in df.columns.values:\n","                    copy[column] = function(df[column])\n","\n","        return copy\n","\n","    def TransformColumnsTypes(self, df, transformations):\n","        columnsList = [transformation[\"columns\"] for transformation in transformations]\n","        functionList = [TypeConverter.GetConvertFunction(transformation[\"type\"], transformation[\"typeArguments\"]) for transformation in transformations]\n","        return self.TransformColumnsByColumn(df, columnsList, functionList)\n","\n","    def mapAllAsDataFrameOperation(self, df, function):\n","        dfFunction = function()\n","        df = dfFunction(df)\n","        return df\n","\n","    def WriteFiles(self, df, path, writer, writerArguments):\n","        if (writer != \"delimited\"):\n","            raise Exception(\"Unsupported writer: \" + writer)\n","\n","        eol = '\\n' if 'eol' in writerArguments and writerArguments[\"eol\"] == 0 else '\\r\\n'\n","        na = writerArguments.get(\"nullReplacement\")\n","        error = writerArguments.get(\"errorReplacement\")\n","        delimiter = writerArguments[\"delimiter\"]\n","\n","        try:\n","            os.makedirs(os.path.dirname(path), exist_ok = True)\n","        except OSError as e:\n","            if e.errno == errno.EACCES:\n","                raise EvaluationError(str(e), 'AccessDenied')\n","            elif e.errno == errno.ENOENT or e.errno == errno.ENOTDIR:\n","                raise EvaluationError(str(e), 'InvalidPath')\n","            else:\n","                raise EvaluationError(str(e), 'Unknown')\n","        try:\n","            with open(path,\n","                        mode = 'w+',\n","                        newline = '',\n","                        encoding = \"utf-8\") as file:\n","                csv_writer = csv.writer(file, delimiter=delimiter, quoting=csv.QUOTE_MINIMAL, lineterminator=eol)\n","                if not df.empty:\n","                    def toString(value):\n","                        if isinstance(value, Exception):\n","                            return error\n","                        if pandas.isnull(value):\n","                            return na\n","                        elif isinstance(value, float):\n","                            if numpy.isinf(value):\n","                                return \"Infinity\" if value > 0 else \"-Infinity\"\n","                            else:\n","                                return int(value) if value.is_integer() else \"{}\".format(value)\n","                        elif isinstance(value, pandas.Timestamp):\n","                            return value.strftime('%Y-%m-%d %H:%M:%S')\n","                        else:\n","                            return str(value).lower() if isinstance(value, bool) else value\n","\n","                    csv_writer.writerow(df.columns)\n","                    for t in df.itertuples(index=False, name=None):\n","                        csv_writer.writerow([toString(value) for value in t])\n","        except UnicodeError as e:\n","            raise EvaluationError(str(e), 'WrongEncodingWrite')\n","        return df\n","\n","    def Join(self, inputLeft, inputRight, kind, leftColumnPrefix, rightColumnPrefix, onColumns):\n","        keysLeft = [onColumn[\"leftColumn\"] for onColumn in onColumns]\n","        keysRight = [onColumn[\"rightColumn\"] for onColumn in onColumns]\n","        prefixLeft = '' if leftColumnPrefix is None else leftColumnPrefix\n","        prefixRight = '' if rightColumnPrefix is None else rightColumnPrefix\n","\n","        if not all(key in inputLeft.columns for key in keysLeft):\n","            raise EvaluationError('', 'MissingLeftKey')\n","        if not all(key in inputRight.columns for key in keysRight):\n","            raise EvaluationError('', 'MissingRightKey')\n","\n","        indicatorColumn = '0_indicator_0'\n","        suffixes = ['', ''] # We don't wnat any dynamic column names (names alter depending on presence of other names).\n","\n","        def performMerge(how):\n","            left = inputLeft.rename(columns = lambda name: prefixLeft + name, copy = False, inplace = False) if prefixLeft != '' else inputLeft\n","            right = inputRight.rename(columns = lambda name: prefixRight + name, copy = False, inplace = False) if prefixRight != '' else inputRight\n","            lKey = [prefixLeft + column for column in keysLeft]\n","            rKey = [prefixRight + column for column in keysRight]\n","\n","            duplicateNames = left.columns.intersection(right.columns)\n","            if len(duplicateNames) > 0:\n","                raise EvaluationError(str.join(', ', ['\"' + name + '\"' for name in duplicateNames]), 'DuplicateColumnName')\n","\n","            return pandas.merge(left, right, left_on=lKey, right_on=rKey, how=how, indicator=indicatorColumn, suffixes=suffixes, sort=False)\n","\n","        def performFilter(df, includes):\n","            return df[df[indicatorColumn].isin(includes)]\n","\n","        if kind == 'empty':\n","            inputLeft = inputLeft.head(0)\n","            inputRight = inputRight.head(0)\n","            df = performMerge('left')\n","        elif kind == 'inner':\n","            df = performMerge('inner')\n","        elif kind == 'leftAnti':\n","            df = performMerge('left')\n","            df = performFilter(df, ['left_only'])\n","        elif kind == 'rightAnti':\n","            df = performMerge('right')\n","            df = performFilter(df, ['right_only'])\n","        elif kind =='leftOuter':\n","            df = performMerge('left')\n","        elif kind == 'rightOuter':\n","            df = performMerge('right')\n","        elif kind == 'fullAnti':\n","            df = performMerge('outer')\n","            df = performFilter(df, ['left_only', 'right_only'])\n","        elif kind == 'fullOuter':\n","            df = performMerge('outer')\n","        else:\n","            raise RuntimeError('Unsupported join type: {0}'.format(kind))\n","\n","        cols = df.columns.tolist()\n","        idxIndicator = cols.index(indicatorColumn)\n","\n","        # change nan to None for mismatch\n","        if kind == 'leftAnti' or kind == 'rightAnti' or kind == 'fullAnti':\n","            leftColCount = inputLeft.shape[1]\n","            leftCols = []\n","            rightCols = []\n","            for i in range(0, leftColCount):\n","                leftCols.append(cols[i])\n","            for i in range(leftColCount, len(cols) - 1):\n","                rightCols.append(cols[i])\n","\n","            for index, row in df.iterrows():\n","                if row[indicatorColumn] == 'left_only':\n","                    for col in rightCols:\n","                        df.set_value(index, col, None)\n","                elif row[indicatorColumn] == 'right_only':\n","                    for col in leftCols:\n","                        df.set_value(index, col, None)\n","\n","        cols = cols[:idxIndicator] + cols[idxIndicator + 1:]\n","        df = df[cols]\n","\n","        return df\n","\n","    @staticmethod\n","    def getWrappedFunction(function, failOnError, valueOnError = None):\n","        def wrappedFunction(row):\n","            with StdioRedirect():\n","                try:\n","                    value = function(row)\n","                except Exception as exception:\n","                    if failOnError == True:\n","                        raise exception\n","                    else:\n","                        # TODO: report row-level error\n","                        value = valueOnError\n","            return value\n","        return wrappedFunction\n","\n","class StdioRedirect():\n","    # redirect stdio calls so that custom code will not impact product running\n","    def __init__(self):\n","        self.stdin = sys.stdin\n","        self.stdout = sys.stdout\n","        self.__stdin__ = sys.__stdin__\n","        self.__stdout__ = sys.__stdout__\n","    def __enter__(self):\n","        inputBuf = io.StringIO()\n","        outputBuf = io.StringIO()\n","        sys.stdin = inputBuf\n","        sys.stdout = outputBuf\n","        sys.__stdin__ = inputBuf\n","        sys.__stdout__ = outputBuf\n","        return (inputBuf, outputBuf)\n","    def __exit__(self, type, value, traceback):\n","        sys.stdin = self.stdin\n","        sys.stdout = self.stdout\n","        sys.__stdin__ = self.__stdin__\n","        sys.__stdout__ = self.__stdout__\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"source":["# 0.1.1703.16012\n","from datetime import datetime\n","ex = Executor.Create()\n","t0 = ex.GetFiles([\"C:\\\\Users\\\\ercharbo\\\\Documents\\\\Ma Formation\\\\PowerBI\\\\demos\\\\Sales and Expenses.xlsx\"])\n","f0 = LoadFunction(\"\"\"import math\n","import numbers\n","import datetime\n","import re\n","import pandas as pd\n","import numpy as np\n","import scipy as sp\n","def func(df):\n","  class Sanitizer():\n","      @classmethod\n","      def makeNamesUnique(cls, names: []) -> []:\n","          # We don't want to use any existing names as new unique names\n","          # and then rename a column that is not duplicated. e.g.\n","          #  X, X, X_1 \n","          # should result in:\n","          #  X, X_2, X_1\n","          # not:\n","          #  X, X_1, X_1_1  <- renamed something that is not duplicated breaking references to X_1\n","          takenNames = set(names)\n","          seenNames = set()\n","          unames = []\n","          baseNameCounters = {}\n","          for name in names:\n","              if name in seenNames:\n","                  name = Sanitizer._makeUnique(baseNameCounters, takenNames, name)\n","                  takenNames.add(name)\n","              else:\n","                  seenNames.add(name)\n","              unames.append(name)\n","          return unames\n","      @classmethod\n","      def _makeUnique(cls, baseNameCounters, nameSet, name: str) -> str:\n","          uniqueName = name\n","          if name in nameSet:\n","              suffix = baseNameCounters[name] if name in baseNameCounters else 1\n","              while True:\n","                  uniqueName = name + \"_\" + str(suffix)\n","                  suffix += 1\n","                  if uniqueName not in nameSet:\n","                      break\n","              baseNameCounters[name] = suffix\n","          return uniqueName\n","      @classmethod\n","      def makeUnique(cls, nameSet, name: str) -> str:\n","          return Sanitizer._makeUnique({}, nameSet, name)\n","      @classmethod\n","      def validateUnique(cls, name: str, existingNames: []):\n","          if not Sanitizer.makeUnique(existingNames, name) == name:\n","              raise ValueError('Column name \"{0}\" already exists.'.format(name), 'InvalidColumnName')\n","  import pandas, random\n","  class FileHelper():\n","      @classmethod\n","      def fileEncodingEnumToPythonEncodingString(cls, args):\n","          if hasattr(args, 'fileEncoding'):\n","              if args.fileEncoding == 0:\n","                  return 'utf-8'\n","              if args.fileEncoding == 1:\n","                  return 'iso-8859-1'\n","              if args.fileEncoding == 2:\n","                  return 'latin-1'\n","              if args.fileEncoding == 3:\n","                  return 'ascii'\n","              if args.fileEncoding == 4:\n","                  return 'utf-16'\n","              if args.fileEncoding == 5:\n","                  return 'utf-32'\n","              if args.fileEncoding == 6:\n","                  return 'utf-8-sig'\n","          else:\n","              return None\n","      @classmethod\n","      def appendRowToRowsPerSampleScheme(cls, rows, row, sampleRandom, maxRows, dataRowIndex):\n","          if sampleRandom:\n","              if len(rows) < maxRows:\n","                  rows.append(row)\n","              else:\n","                  r = int(random.random() * (dataRowIndex + 1))\n","                  if r < maxRows:\n","                      rows[r] = row\n","          elif maxRows:\n","              rows.append(row)\n","              if len(rows) >= maxRows:\n","                  return False\n","          else:\n","              rows.append(row)\n","          return True\n","      @classmethod\n","      def createHeaders(cls, headerRow, numColumns):\n","          headers = [hrc or 'Column' + str(hri + 1) for hri, hrc in enumerate(headerRow)] + (['Column' + str(c + 1 + len(headerRow)) for c in range(0, numColumns - len(headerRow))])\n","          headers = Sanitizer.makeNamesUnique(headers)\n","          return headers\n","      @classmethod\n","      def createDataFrameAndHeader(cls, headerRow, rows):\n","          if len(headerRow) > 0 and len(rows) == 0:\n","              rows.append(headerRow)\n","              headerRow = []\n","          df = pandas.DataFrame(rows)\n","          headers = FileHelper.createHeaders(headerRow, len(df.columns))\n","          # compensate for a Pandas issue where the header row can't be wider than the dataframe\n","          # if so, pad the first row with blanks, then re-create dataframe\n","          if len(headers) > len(df.columns):\n","              rows[0].extend([None] * (len(headers) - len(rows[0])))\n","              df = pandas.DataFrame(rows)\n","          df.columns = headers\n","          return df\n","  class EvaluationError(Exception):\n","      def __init__(self, message, errorCode):\n","          super(EvaluationError, self).__init__(message)\n","          self.errorCode = errorCode\n","  import xlrd, datetime\n","  from collections import namedtuple\n","  def readExcel(dataFrame, args):\n","      useColumnHeaders = None if args.useColumnHeaders == False else 0\n","      skipRows = args.skipRows if hasattr(args, 'skipRows') else None           \n","      headerRow = []\n","      rows = []\n","      path = dataFrame.iloc[0]['Path'] # For now we don't support mulltifile excel load.\n","      with xlrd.open_workbook(path) as xlsx:\n","          if args.sheetName.isdigit():\n","              sheet = xlsx.sheet_by_index(int(args.sheetName))\n","          else:\n","              sheet = xlsx.sheet_by_name(args.sheetName)\n","          dataRowIndex = 0\n","          for i, row in enumerate(sheet.get_rows()):\n","              if skipRows != None and i < skipRows:\n","                  continue\n","              # fix up types\n","              #   XL_CELL_EMPTY       0   empty string\n","              #   XL_CELL_TEXT        1   a Unicode string\n","              #   XL_CELL_NUMBER      2   float\n","              #   XL_CELL_DATE        3   float\n","              #   XL_CELL_BOOLEAN     4   int; 1 means TRUE, 0 means FALSE\n","              #   XL_CELL_ERROR       5   int representing internal Excel codes\n","              for j, cell in enumerate(row):\n","                  if cell.ctype == 0 or cell.ctype == 1:\n","                      row[j] = cell.value if isinstance(cell.value, str) else str(cell.value)\n","                  elif cell.ctype == 2:\n","                      row[j] = str(int(cell.value)) if cell.value.is_integer() else str(cell.value)\n","                  elif cell.ctype == 3:\n","                      try:\n","                          dt_tuple = xlrd.xldate_as_tuple(cell.value, xlsx.datemode)\n","                          row[j] = str(datetime.datetime(*dt_tuple))\n","                      except:\n","                          row[j] = str(cell.value)\n","                  elif cell.ctype == 4:\n","                      row[j] = 'true' if cell.value == 1 else 'false'\n","                  elif cell.ctype == 5:\n","                      row[j] = xlrd.error_text_from_code[cell.value]\n","              if useColumnHeaders != None and useColumnHeaders == dataRowIndex:\n","                  headerRow = row\n","              else:\n","                  continueProcessingRows = FileHelper.appendRowToRowsPerSampleScheme(rows, row, None, None, dataRowIndex)\n","                  if continueProcessingRows == False:\n","                      break\n","              dataRowIndex = dataRowIndex + 1\n","      dfc = FileHelper.createDataFrameAndHeader(headerRow, rows)\n","      return dfc\n","  return readExcel(df, namedtuple('args', ['useColumnHeaders', 'skipRows', 'sheetName'])(False, None, '0'))\n","\"\"\", \"func\")\n","t1 = ex.mapAllAsDataFrameOperation(t0, f0)\n","def f1():\n","    return CleanStringNumberTransform(\",\")\n","t2 = ex.TransformColumns(t1, [{\"columns\":[\"Column11\"]}], [f1])\n","t3 = ex.TransformColumnsTypes(t2, [{\"columns\":[\"Column11\"],\"type\":\"number\",\"typeArguments\":{}}])\n","def f2():\n","    return CleanStringNumberTransform(\",\")\n","t4 = ex.TransformColumns(t3, [{\"columns\":[\"Column12\"]}], [f2])\n","t5 = ex.TransformColumnsTypes(t4, [{\"columns\":[\"Column12\"],\"type\":\"number\",\"typeArguments\":{}}])\n","def f3():\n","    return CleanStringNumberTransform(\",\")\n","t6 = ex.TransformColumns(t5, [{\"columns\":[\"Column13\"]}], [f3])\n","t7 = ex.TransformColumnsTypes(t6, [{\"columns\":[\"Column13\"],\"type\":\"number\",\"typeArguments\":{}}])\n","def f4():\n","    return CleanStringNumberTransform(\",\")\n","t8 = ex.TransformColumns(t7, [{\"columns\":[\"Column14\",\"Column15\",\"Column16\",\"Column17\",\"Column18\",\"Column19\"]}], [f4])\n","t9 = ex.TransformColumnsTypes(t8, [{\"columns\":[\"Column14\",\"Column15\",\"Column16\",\"Column17\",\"Column18\",\"Column19\"],\"type\":\"number\",\"typeArguments\":{}}])\n","t10 = ex.PromoteHeaders(t9)\n","f5 = LoadFunction(\"\"\"def func(value):\n","  try:\n","      return value if value is None or isinstance(value, Exception) else round(value, 2)\n","  except TypeError:\n","      return Exception({'message':'Cannot apply adjust precision function to not a number', '__errorCode__':'Microsoft.DPrep.ErrorValues.InvalidStringType', 'originalValue' : str(value)})\"\"\", \"func\")\n","t11 = ex.TransformColumns(t10, [{\"columns\":[\"Column13\"]}], [f5])\n","f6 = LoadFunction(\"\"\"def func(value):\n","  try:\n","      return value if value is None or isinstance(value, Exception) else round(value, 2)\n","  except TypeError:\n","      return Exception({'message':'Cannot apply adjust precision function to not a number', '__errorCode__':'Microsoft.DPrep.ErrorValues.InvalidStringType', 'originalValue' : str(value)})\"\"\", \"func\")\n","t12 = ex.TransformColumns(t11, [{\"columns\":[\"Column16\"]}], [f6])\n","def f7():\n","    return CleanStringNumberTransform(\",\")\n","t13 = ex.TransformColumns(t12, [{\"columns\":[\"Fiscal Month\"]}], [f7])\n","t14 = ex.TransformColumnsTypes(t13, [{\"columns\":[\"Fiscal Month\"],\"type\":\"number\",\"typeArguments\":{}}])\n","def f8():\n","    def _86228c792fba4d51b61f8bdca6486d12(row):\n","        return Value_LE(row[\"Fiscal Month\"], 6)\n","    return _86228c792fba4d51b61f8bdca6486d12\n","t15 = ex.Filter(t14, f8)\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}